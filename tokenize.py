# -*- encoding: utf-8 -*-

'''
## Heiyeluren Black Transformer ##

Heiyeluren Black Transformer

author: heiyeluren
date: 2023/7/17
site: github.com/heiyeluren

description:

black-transformer æ˜¯ä¸€ä¸ªè½»é‡çº§æ¨¡æ‹ŸTransformeræ¨¡å‹å®ç°çš„æ¦‚è¦ä»£ç ï¼Œç”¨äºäº†è§£æ•´ä¸ªTransformerå·¥ä½œæœºåˆ¶

'''

import sys
import os
import json
import math
import regex
import six
import requests
from builtins import str
from itertools import chain

# Black Tokenizer
class Tokenize(object):

    #
    # Inner function
    #

    # tokenizer = BlackTokenize()
    # åˆå§‹åŒ–ç±»æ‰€éœ€è¦çš„åŸºæœ¬èµ„æº
    def __init__(self):
        
        # base data file
        self._TOKEN_ENCODER_FILE = os.path.join(os.path.dirname(__file__), 'encoder.json')
        self._TOKEN_VOCAB_FILE   = os.path.join(os.path.dirname(__file__), "vocab.bpe")
        self._ENCODER_JSON_URL = "https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/encoder.json"
        self._VOCAB_BPE_URL   = "https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/vocab.bpe"
        
        # base strucutre
        self._REGEX_PATTERN     = r"'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"
        self._DEFAULT_ENCODING  = "utf-8"
        self._cache             = {}
        self._byte_encoder      = {}
        self._byte_decoder      = {}
        self._bpe_merges        = {}
        self._bpe_ranks         = {}
        self._encoder           = {}
        self._decoder           = {}
        self._regex_compiled    = None

        # fetch encoder.json and vocab.bpe file
        self._fetch_encode_bpe_file()

        # init tokenizer
        self._bpe_merges        = self._init_bpe_merges()
        self._bpe_ranks         = self._init_bpe_rank(self._bpe_merges, range(0, len(self._bpe_merges)))
        self._encoder           = self._init_encoder()
        self._decoder           = self._init_decoder()       #{v: k for k,v in self._encoder.items()}
        self._byte_encoder      = self._init_bytes_encoder()
        self._byte_decoder      = self._init_byte_decoder()  #{v: k for k,v in self._byte_encoder.items()}
        self._regex_compiled    = self._init_regex_compiled()
 

    # è·å–GPT-2å®˜æ–¹çš„encoder.jsonå’Œvocab.bpeæ–‡ä»¶
    def _fetch_encode_bpe_file(self, encoder_json_url=None, 
                            vocab_bpe_url=None):
        if encoder_json_url is None:
            encoder_json_url = self._ENCODER_JSON_URL
        if vocab_bpe_url is None:
            vocab_bpe_url = self._VOCAB_BPE_URL
        
        if os.path.exists(self._TOKEN_ENCODER_FILE) \
            and os.path.exists(self._TOKEN_VOCAB_FILE) \
            and os.path.getsize(self._TOKEN_ENCODER_FILE) > 0 \
            and os.path.getsize(self._TOKEN_VOCAB_FILE) > 0:
            return True
        
        encoder_json = requests.get(encoder_json_url)
        if encoder_json.status_code != 200 and len(encoder_json.text) < 0:
            raise Exception("Http get "+ encoder_json_url +" failed.")
            return False
        
        vocab_bpe = requests.get(vocab_bpe_url)
        if vocab_bpe.status_code != 200 and len(vocab_bpe.text) < 0:
            raise Exception("Http get "+ vocab_bpe_url +" failed.")
            return False

        with open(self._TOKEN_ENCODER_FILE, "wb") as f:
            f.write(encoder_json.content)
        
        with open(self._TOKEN_VOCAB_FILE, "wb") as f:
            f.write(vocab_bpe.content)

        return True        
    
    # initialize bpe encoder
    # è¯»å–ä¸€ä¸ªå­˜å‚¨äº† BPE ç¼–ç è§„åˆ™çš„æ–‡ä»¶ï¼Œè§£æå‡ºè¿™äº›è§„åˆ™ï¼Œå¹¶ä»¥åˆ—è¡¨çš„å½¢å¼è¿”å›
    def _init_bpe_merges(self):
        if len(self._bpe_merges) > 0: return self._bpe_merges
        with open(self._TOKEN_VOCAB_FILE, "r") as f:
            bpe_lines = f.readlines()    
            sliced = bpe_lines[1:len(bpe_lines)-1]
            bpe_merges = [regex.split(r"(\s+)", s) for s in sliced]
            final_merges = []
            for merge in bpe_merges:
                final_merges.append([m for m in merge if len(m.strip()) > 0])
            # self.bpe_merges = final_merges
            # return self.bpe_merges
            return final_merges
        
    # initialize bpe rank
    # å°† BPE ç¼–ç è§„åˆ™è½¬æ¢æˆä¸€ä¸ªæ˜ å°„è¡¨ï¼Œè¯¥æ˜ å°„è¡¨å¯ä»¥å°†ç”± BPE ç¼–ç è§„åˆ™åˆå¹¶åçš„è¯æ±‡è½¬æ¢æˆå®ƒä»¬åœ¨ BPE ç¼–ç åºåˆ—ä¸­çš„åºå·
    # å°†BFEç¼–ç ç±»ä¼¼ [['hel', 'lo'], ['wor', 'ld']] è¿›è¡Œå‡½æ•°å¤„ç†ä»¥åç”Ÿæˆæ˜ å°„è¡¨ { 'hel,lo': 0, 'wor,ld': 1 }
    def _init_bpe_rank(self, x, y):
        if len(self._bpe_ranks) > 0: return self._bpe_ranks
        result = {}
        for i in y:
            key = ','.join(x[i])
            if not isinstance(key, str):
                key = key.decode(self._DEFAULT_ENCODING)
            result[key] = y[i]
        return result    

    # load encoder json file 
    # å°†å·²ç»è®­ç»ƒå¥½ä¸”å­˜å‚¨åœ¨æ–‡ä»¶ä¸­çš„ç¼–ç å™¨è¯»å–åˆ°å†…å­˜ä¸­
    def _init_encoder(self):
        if self._encoder != {}: return self._encoder
        with open(self._TOKEN_ENCODER_FILE, 'r') as f:
            encoder = json.load(f)
            return encoder    
    
    # from encoder trans to decoder
    # ç”Ÿæˆä¸€ä¸ªè§£ç å™¨ï¼Œå³å°†ç¼–ç å™¨ä¸­çš„æ¯ä¸ªç¼–ç ç¬¦å·ä¸å…¶å¯¹åº”çš„åŸå§‹è¯æ±‡å»ºç«‹ä¸€ä¸ªåå‘æ˜ å°„è¡¨ã€‚ç±»ä¼¼äºæŠŠ { 'a': 0, 'b': 1, 'c': 2] è½¬æˆ {0: 'a', 1: 'b', 2: 'c'}
    def _init_decoder(self):
        if self._decoder != {}: return self._decoder
        return {v: k for k,v in self._encoder.items()}
    
    # initialize special byte encoder
    # ç”Ÿæˆä¸€ä¸ªç¼–ç æ˜ å°„è¡¨ï¼Œè¯¥æ˜ å°„è¡¨å¯ä»¥å°†ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ç¼–ç æˆæŒ‰ç…§ç‰¹å®šè§„åˆ™è½¬æ¢åçš„å­—ç¬¦ä¸²ï¼ŒæŒ‡å®šæ‰¹é‡ç‰¹æ®Šè¿›è¡Œç¼–ç ï¼Œç»“æ„ç±»ä¼¼ {'33': '!', '34': '"', '35': '#' ... }
    def _init_bytes_encoder(self):
        if self._byte_encoder != {}: return self._byte_encoder
        # include charï¼š
        # !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
        # Â¡Â¢Â£Â¤Â¥Â¦Â§Â¨Â©ÂªÂ«Â¬ 
        # Â­Â®Â¯Â°Â±Â²Â³Â´ÂµÂ¶Â·Â¸Â¹ÂºÂ»Â¼Â½Â¾Â¿Ã€ÃÃ‚ÃƒÃ„Ã…Ã†Ã‡ÃˆÃ‰ÃŠÃ‹ÃŒÃÃÃÃÃ‘Ã’Ã“Ã”Ã•Ã–Ã—Ã˜Ã™ÃšÃ›ÃœÃÃÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã·Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ã¿
        bs = list(chain(self._range(self._ord('!'), self._ord('~') + 1), self._range(self._ord('Â¡'), self._ord('Â¬') + 1), self._range(self._ord('Â®'), self._ord('Ã¿') + 1)))
        cs = bs[:]
        n = 0
        b = 0
        while b < 2 ** 8:
            if not b in bs:
                bs.append(b)
                cs.append(2 ** 8 + n)
                n += 1
            b += 1

        cs = list(map(lambda x: six.unichr(x), cs))
        result = {}
        for i in range(len(bs)):
            result[str(bs[i])] = cs[i]
        self._byte_encoder = result
        return self._byte_encoder
    
    # from byte encoder trans to byte decoder
    # å°†ç¼–ç æ˜ å°„è¡¨ç”Ÿæˆä¸€ä¸ªè§£ç æ˜ å°„è¡¨ï¼Œè§£ç æ˜ å°„è¡¨å¯ä»¥å°†ä¸€ä¸ªæ•´æ•°åˆ—è¡¨è§£ç æˆåŸå§‹çš„å­—ç¬¦ä¸²ï¼Œå°† {72: 'H'} å˜æˆ {'H': 72}
    def _init_byte_decoder(self):
        if self._byte_decoder != {}: return self._byte_decoder
        return {v: k for k,v in self._init_bytes_encoder().items()}    

    # use utf-8 encode string trans to bytes array
    # å°†ä¸€ä¸ªå­—ç¬¦ä¸²ç¼–ç æˆä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼Œå…¶ä¸­åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸² token ä¸­æ¯ä¸ªå­—ç¬¦çš„ ASCII ç 
    def _encode_string(self, token):
        return [str(t) for t in list(bytearray(token.encode(self._DEFAULT_ENCODING)))]

    # è¿”å›ä¸€ä¸ªä» x å¼€å§‹çš„æ•´æ•°åˆ—è¡¨ï¼Œå…¶ä¸­èµ·å§‹æ•°å­—æ˜¯ xï¼Œæœ€å¤§æ•°å­—ä¸º y-1
    def _range(self, x, y):
        res = [val for val in range(y)][x:]
        return res

    # use utf-8 ord char 
    # å°†ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–è€…å­—èŠ‚ä¸²è½¬æ¢æˆå¯¹åº”çš„ Unicode ç ç‚¹æ•°å€¼ï¼Œç”¨äºå°†å­—ç¬¦ä¸²æˆ–è€…å­—èŠ‚ä¸²ç¼–ç æˆæ•°å­—å½¢å¼
    def _ord(self, x):
        if not isinstance(x, str):
            x = x.decode(self._DEFAULT_ENCODING)
        res = ord(x[0])
        return res

    # Pairwise combination of adjacent characters in a word into one element
    # å°†ä¸€ä¸ªå•è¯ä¸­ç›¸é‚»çš„å­—ç¬¦ä¸¤ä¸¤ç»„åˆæˆä¸€ä¸ªå…ƒç´ 
    def _get_pairs(self, word):
        pairs = []
        prev_char = word[0]
        for i in range(1, len(word)):
            ch = word[i]
            pairs.append([prev_char, ch])
            prev_char = ch
        return pairs
    
    # initialize regex compiled
    # ç”Ÿæˆä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼å¯¹è±¡ï¼Œä¸»è¦æ‹†è§£è¿™äº›å­—ç¬¦
    '''
        's, 't, 're, 've, 'm, 'll, 'dï¼šè¿ç¼€è¯ï¼Œå¦‚ you'veã€you'reã€I'd ç­‰ï¼›
        \p{L}+ï¼šä¸€ä¸ªæˆ–å¤šä¸ªè¿ç»­çš„ Unicode å­—æ¯å­—ç¬¦ï¼›
        \p{N}+ï¼šä¸€ä¸ªæˆ–å¤šä¸ªè¿ç»­çš„ Unicode æ•°å­—å­—ç¬¦ï¼›
        [^\s\p{L}\p{N}]+ï¼šä¸€ä¸ªæˆ–å¤šä¸ªè¿ç»­çš„éç©ºæ ¼ã€éå­—æ¯ã€éæ•°å­—çš„ Unicode å­—ç¬¦ï¼›
        \s+(?!\S)ï¼šä¸€ä¸ªæˆ–å¤šä¸ªè¿ç»­çš„ç©ºæ ¼å­—ç¬¦ï¼Œä¸”è¿™äº›ç©ºæ ¼å­—ç¬¦åé¢æ²¡æœ‰éç©ºæ ¼å­—ç¬¦ï¼›
        \s+ï¼šä¸€ä¸ªæˆ–å¤šä¸ªè¿ç»­çš„ç©ºæ ¼å­—ç¬¦
    '''
    def _init_regex_compiled(self):
        if self._regex_compiled != None: return self._regex_compiled
        # self._REGEX_PATTERN     = r"'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"
        regex_compiled = regex.compile(self._REGEX_PATTERN, regex.UNICODE)
        return regex_compiled
    
    #
    # Core API
    #

    
    # use BPE algorithm encode input word or phrase
    # ä½¿ç”¨ BPEï¼ˆByte Pair Encodingï¼‰ç®—æ³•å°†è¾“å…¥çš„å•è¯æˆ–è¯ç»„è¿›è¡Œç¼–ç , è¯¥æ–¹æ³•åªè´Ÿè´£å¯¹å•ä¸ªå•è¯æˆ–è¯ç»„è¿›è¡Œ BPE ç¼–ç ï¼Œå¦‚æœè¦å¯¹ä¸€ç»„æ–‡æœ¬æ•°æ®è¿›è¡Œ BPE ç¼–ç ï¼Œéœ€è¦è°ƒç”¨ _bpe_batch æ–¹æ³•
    """
    BPE æ˜¯ä¸€ç§å‹ç¼©ç®—æ³•ï¼Œç”¨äºå°†æ–‡æœ¬æ•°æ®ä¸­å¸¸è§çš„è¿ç»­å­—ç¬¦åºåˆ—åˆå¹¶æˆå•ä¸ªå­—ç¬¦ï¼Œä»¥å‡å°‘è¯æ±‡é‡å¹¶æé«˜å‹ç¼©æ•ˆç‡

    1. åŸºäºè®­ç»ƒæ•°æ®ç”Ÿæˆ BPE ç è¡¨ï¼Œå³ç”Ÿæˆå¸¸è§å­—æ¯æˆ–å­—ç¬¦ä¸²çš„ç»„åˆï¼Œå¹¶ç»™ç»„åˆç¼–ç ä¸€ä¸ªæ•´æ•°ä½œä¸ºæ ‡è¯†ç¬¦ã€‚
    2. å°†æ–‡æœ¬ä¸­æ‰€æœ‰çš„å•è¯åˆ’åˆ†æˆå­—ç¬¦æˆ–è€…å­—ç¬¦ç»„æˆçš„å­ä¸²ã€‚
    3. åœ¨æ‰€æœ‰å•è¯ä¸­æ‰¾å‡ºå‡ºç°æ¬¡æ•°æœ€å¤šçš„å­—ç¬¦æˆ–è€…å­—ç¬¦ç»„åˆï¼Œå°†è¿™ä¸ªå­—ç¬¦æˆ–è€…å­—ç¬¦ç»„åˆå½“åšä¸€ä¸ªæ–°çš„å­—ç¬¦æ¥æ›¿ä»£åŸæœ‰å•è¯ä¸­çš„è¿™ä¸ªå­—ç¬¦æˆ–è€…å­—ç¬¦ç»„åˆã€‚å¹¶åœ¨ç¼–ç è¡¨ä¸­æ·»åŠ è¿™ä¸ªå­—ç¬¦æˆ–è€…å­—ç¬¦ç»„åˆçš„ç¼–ç ã€‚
    3. é‡å¤æ­¥éª¤ 3 ç›´åˆ°è¾¾åˆ°é¢„è®¾çš„ BPE ç¼–ç æ¬¡æ•°æˆ–è€…åˆ°è¾¾æœ€å°è¯é¢‘ã€‚
    """
    def _bpe(self, token, bpe_ranks):
        if token in self._cache:
            return self._cache[token]
        word = list(token)
        pairs = self._get_pairs(word)
        if not pairs:
            return token

        while True:
            min_pairs = {}
            for pair in pairs:
                pair_key = ','.join(pair)
                rank = bpe_ranks.get(pair_key, float("nan"))
                min_pairs[10e10 if math.isnan(rank) else rank] = pair_key
            bigram = min_pairs[min(map(int, min_pairs.keys()))]
            if not bigram in bpe_ranks:
                break
            bigram = bigram.split(',', 1)
            first = bigram[0]
            second = bigram[1]
            new_word = []
            i = 0

            while i < len(word):
                j = -1
                try:
                    j = word.index(first, i)
                except:
                    pass
                if j == -1:
                    new_word.extend(word[i:])
                    break
                new_word.extend(word[i:j])
                i = j
                if word[i] == first and i < len(word)-1 and word[i+1] == second:
                    new_word.append(first+second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1

            word = new_word
            if len(word) == 1:
                break
            pairs = self._get_pairs(word)
        
        word = ' '.join(word)
        self._cache[token] = word
        return word
    
    # use BPE algorithm encode input word or phrase (batch)
    # æ‰¹é‡å¤„ç† bpe ç¼–ç  (å¯ä»¥ä¼ é€’å¤šä¸ªtokenè¿›è¡Œç¼–ç )
    def _bpe_batch(self, tokens, bpe_ranks):
        result = []
        for token in tokens:
            if token in self._cache:
                result.append(self._cache[token])
            else:
                code = self._bpe(token, bpe_ranks)
                result.append(code)
                self._cache[token] = code
        return result

    # API: BlackTokenizer.encode(text)
    # å°†ä¸€ä¸ªå­—ç¬¦ä¸²ç¼–ç æˆä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ˆtokensï¼‰
    def encode(self, text):
        """ 
            Transforms a string into an array of tokens 
            :param text: string to be encoded
            :type text: str
            :returns: an array of ints (tokens)
        """
        if not isinstance(text, str):
            text = text.decode(self._DEFAULT_ENCODING)    
        bpe_tokens = []
        matches = self._regex_compiled.findall(text)
        for token in matches:
            token = ''.join([self._byte_encoder[x] for x in self._encode_string(token)])
            new_tokens = [self._encoder[x] for x in self._bpe(token, self._bpe_ranks).split(' ')]
            bpe_tokens.extend(new_tokens)
        return bpe_tokens

    # API: BlackTokenizer.decode(tokens)
    # å°†è¾“å…¥çš„æ•´æ•°åˆ—è¡¨ tokens è½¬æ¢æˆåŸå§‹å­—ç¬¦ä¸²
    def decode(self, tokens):
        """ 
            Transforms back an array of tokens into the original string
            :param tokens: an array of ints
            :type tokens: list
            :returns: the original text which was encoded before
        """
        text = ''.join([self._decoder[x] for x in tokens])
        textarr = [int(self._byte_decoder[x]) for x in list(text)]
        text = bytearray(textarr).decode("utf-8")
        return text

    # API: BlackTokenizer.count_tokens(text)
    # 
    def count_tokens(self, text):
        """ 
            Returns an integer representing the tokens count of a given string 
            :param text: string to count tokens from
            :type text: str
            :returns: int representing the tokens count
            
        """
        encoded = self.encode(text)
        return len(encoded)  

    # API: BlackTokenizer.get_tokens(text)
    # ä»è¾“å…¥çš„å­—ç¬¦ä¸² text ä¸­è·å– tokens åˆ—è¡¨ã€‚ tokens åˆ—è¡¨æ˜¯ç”±ä¸€ä¸ªæˆ–å¤šä¸ªå•è¯/å­—ç¬¦ç»„æˆçš„åˆ—è¡¨
    def get_token_list(self, text):
        """ 
            Returns an array of tokens from a given string 
            :param text: string to get tokens from
            :type text: str
            :returns: an array of tokens
        """
        if not isinstance(text, str):
            text = text.decode(self._DEFAULT_ENCODING)
        bpe_tokens = []
        matches = self._regex_compiled.findall(text)
        for token in matches:
            new_tokens = [x for x in token.split()]
            bpe_tokens.extend(new_tokens)
        return bpe_tokens            
           
   
'''
Black Tokenizer Test code

Time: 2023/4/29
'''

# seqs = [
#     'æˆ‘çš„åå­—å«åšé»‘å¤œè·¯äºº', 
#     'My name is Black',
#     "æˆ‘çš„nicknameå«heiyeluren",
#     "ã¯ã˜ã‚ã¾ã—ã¦",
#     "ì˜ ë¶€íƒ ë“œë¦½ë‹ˆë‹¤",
#     "Ğ”Ğ¾ ÑĞ²Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ!",
#     "ğŸ˜ŠğŸ˜ğŸ˜„ğŸ˜‰ğŸ˜†ğŸ¤ğŸ‘‹",
#     "ä»Šå¤©çš„çŠ¶æ€å¾ˆhappyï¼Œè¡¨æƒ…æ˜¯ğŸ˜",
# ]

# print('\n------------------BlackTokenize Test------------------')

# tk = Tokenize()
# for seq in seqs:
#     token_list = tk.get_token_list(seq)
#     # print('Text:', seq, ' => Tokens:', tokens)
#     enc_seq = tk.encode(seq)
#     # continue
#     dec_seq = tk.decode(enc_seq)
#     token_count = tk.count_tokens(seq)
#     print( 'RawText:', seq, ' => TokenList:', token_list, ' => TokenIDs', enc_seq, ' => TokenCount:', token_count, '=> DecodeText:', dec_seq)

# print('------------------BlackTokenize Test------------------\n')

